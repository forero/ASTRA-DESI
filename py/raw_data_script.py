# -*- coding: utf-8 -*-
"""Raw_data_script_DR1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f7ogFXxitzYgP1HcBqquflZnAEz3PoCf
"""

import os
import pandas as pd
import numpy as np
from astropy.io import fits
from astropy.coordinates import SkyCoord
from astropy.cosmology import Planck18
import astropy.units as u
from astropy.table import Table
from tqdm import tqdm
import time
import random

base_url = "https://data.desi.lbl.gov/public/dr1/survey/catalogs/dr1/LSS/iron/LSScats/v1.5/"
tracers = ['BGS_ANY', 'ELG_LOPnotqso', 'LRG', 'QSO']
real_suffix = {'N': '_NGC_clustering.dat.fits'}
random_suffix = {'N': '_NGC_{}_clustering.ran.fits'}
n_random_files = 2
n_random = 10
n_zones = 2
cols_to_select = ['TARGETID','RA', 'DEC', 'Z']
output_dir = "01_create_raw"
os.makedirs(output_dir, exist_ok=True)

real_cache = {}
random_cache = {}

def load_fits_file_from_url(url, columns):
    while True:
        try:
            table = Table.read(url)
            break
        except Exception as e:
            wait = random.uniform(5, 10)
            print(f"Error downloading {url}: {e}\nRetrying...\n")
            time.sleep(wait)

    # Select only necessary columns
    cols_to_select = [col for col in columns if col in table.colnames]
    table = table[cols_to_select]
    df = table.to_pandas()

    # Apply zone-based filters
    # mask_ngc1 = (
    #     (df['RA'] > 110) & (df['RA'] < 260) &
    #     (df['DEC'] > -10) & (df['DEC'] < 8) &
    #     (df['Z'] > 0.4) & (df['Z'] < 0.8)
    # )

    # mask_ngc2 = (
    #     (df['RA'] > 180) & (df['RA'] < 260) &
    #     (df['DEC'] > 30) & (df['DEC'] < 40) &
    #     (df['Z'] > 0.4) & (df['Z'] < 0.8)
    # )

    mask_ngc1 = (
    (df['RA'] > 110) & (df['RA'] < 260) &
    (df['DEC'] > -10) & (df['DEC'] < 8)
)

    mask_ngc2 = (
    (df['RA'] > 180) & (df['RA'] < 260) &
    (df['DEC'] > 30) & (df['DEC'] < 40)
)

    df_ngc1 = df[mask_ngc1].copy()
    df_ngc1['ZONE'] = 'NGC1'

    df_ngc2 = df[mask_ngc2].copy()
    df_ngc2['ZONE'] = 'NGC2'

    return pd.concat([df_ngc1, df_ngc2], ignore_index=True)

def compute_cartesian(df):
    df = df.copy()
    comoving_distance = Planck18.comoving_distance(df['Z'].values)
    coords = SkyCoord(ra=df['RA'].values * u.deg, dec=df['DEC'].values * u.deg, distance=comoving_distance)
    df['XCART'] = coords.cartesian.x.value
    df['YCART'] = coords.cartesian.y.value
    df['ZCART'] = coords.cartesian.z.value
    return df

def process_real(tracer, zone):
    df = real_cache[tracer]
    df = df[df['ZONE'] == zone].copy()
    df = compute_cartesian(df)
    df['TRACERTYPE'] = np.full(len(df), tracer + "_DATA", dtype='S14')
    df['RANDITER'] = -1
    return df

def get_real_count(tracer, zone):
    df = real_cache[tracer]
    return len(df[df['ZONE'] == zone])

def generate_randoms_for_zone(tracer, zone):
    # Access the 18 preloaded random files for that tracer
    zone_random_dfs = [df[df['ZONE'] == zone] for df in random_cache[tracer].values()]

    # Amount of real points for this zone
    real_count = get_real_count(tracer, zone)
    all_randoms = []

    used_ran = set()
    ran_file_ids = list(range(len(zone_random_dfs)))

    for j in range(n_random):
        if len(used_ran) == len(ran_file_ids):
            used_ran = set()

        candidates = [i for i in ran_file_ids if i not in used_ran]
        ran_file = random.Random(j).choice(candidates)
        used_ran.add(ran_file)

        df = zone_random_dfs[ran_file]
        sample_df = df.sample(n=real_count, replace=False, random_state=j).reset_index(drop=True)
        sample_df = compute_cartesian(sample_df)
        sample_df['TRACERTYPE'] = np.full(len(sample_df), tracer + "_RAND", dtype='S14')
        sample_df['RANDITER'] = j
        all_randoms.append(sample_df)

        print(f"â†’ [Zone {zone} | Tracer {tracer}] random #{j+1:02d} uses file #{ran_file}")

    return pd.concat(all_randoms, ignore_index=True)

# --- MAIN EXECUTION ---

print("Preloading all real and random files...")

zones = ['NGC1', 'NGC2']

# Preload real data
for tracer in tracers:
    url = base_url + tracer + "_NGC_clustering.dat.fits"
    print(f"Downloading real data: {url}")
    df = load_fits_file_from_url(url, ['TARGETID', 'RA', 'DEC', 'Z'])
    real_cache[tracer] = df

# Preload random data
for tracer in tracers:
    random_cache[tracer] = {}
    for i in range(n_random_files):
        url = base_url + tracer + f"_NGC_{i}_clustering.ran.fits"
        print(f"Downloading random file: {url}")
        df = load_fits_file_from_url(url, ['TARGETID', 'RA', 'DEC', 'Z'])
        random_cache[tracer][i] = df

# --- PROCESSING LOOP ---
for zone in tqdm(zones, desc="Zones"):
    all_dfs = []
    for tracer in tracers:
        print(f"\nProcessing tracer: {tracer} | Zone: {zone}")
        real_df = process_real(tracer, zone)
        rand_df = generate_randoms_for_zone(tracer, zone)
        all_dfs.extend([real_df, rand_df])

    combined = pd.concat(all_dfs, ignore_index=True)
    combined = combined[['TARGETID', 'TRACERTYPE', 'RANDITER', 'RA', 'DEC', 'Z', 'XCART', 'YCART', 'ZCART']]
    combined['TRACERTYPE'] = combined['TRACERTYPE'].astype(str).values.astype('S14')

    output_path = os.path.join(output_dir, f"{zone.lower()}.fits.gz")
    fits.writeto(output_path, combined.to_records(index=False), overwrite=True)
    print(f"Saved: {output_path}")

